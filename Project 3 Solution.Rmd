---
title: "Project 3 Solution"
author: "Ananya Bhaktaram"
date: "`r Sys.Date()`"
output: html_document
---
# Background

**Due date: October 20th at 11:59pm**

The goal of this assignment is to practice wrangling special data types (including dates, character strings, and factors) and visualizing results while practicing our tidyverse skills.

# Load data

The datasets for this part of the assignment comes from [TidyTuesday](https://www.tidytuesday.com).

Data dictionary available here:

-   <https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-29>

Specifically, we will explore album sales and lyrics from two artists (Beyoncé and Taylor Swift), The data are available from TidyTuesday from September 2020, which I have provided for you below:

```{r, eval=FALSE}
b_lyrics <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv")
ts_lyrics <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/taylor_swift_lyrics.csv")
sales <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/sales.csv")
```

However, to avoid re-downloading data, we will check to see if those files already exist using an `if()` statement:

```{r, message=FALSE}
library("here")
rds_files <- c("b_lyrics.RDS", "ts_lyrics.RDS", "sales.RDS")
## Check whether we have all 3 files
if (any(!file.exists(here("data", rds_files)))) {
    ## If we don't, then download the data
    b_lyrics <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv")
    ts_lyrics <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/taylor_swift_lyrics.csv")
    sales <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/sales.csv")

    ## Then save the data objects to RDS files
    saveRDS(b_lyrics, file = here("data", "b_lyrics.RDS"))
    saveRDS(ts_lyrics, file = here("data", "ts_lyrics.RDS"))
    saveRDS(sales, file = here("data", "sales.RDS"))
}
```

::: callout-note
The above code will only run if it cannot find the path to the `b_lyrics.RDS` on your computer. Then, we can just read in these files every time we knit the R Markdown, instead of re-downloading them every time.
:::

Let's load the datasets

```{r, eval=TRUE, message=FALSE}
b_lyrics <- readRDS(here("data", "b_lyrics.RDS"))
ts_lyrics <- readRDS(here("data", "ts_lyrics.RDS"))
sales <- readRDS(here("data", "sales.RDS"))
```

# Part 1: Explore album sales

In this section, the goal is to explore the sales of studio albums from Beyoncé and Taylor Swift.

**Notes**

-   In each of the subsections below that ask you to create a plot, you must create a title, subtitle, x-axis label, and y-axis label with units where applicable. For example, if your axis says "sales" as an axis label, change it to "sales (in millions)".

## Part 1A

In this section, we will do some data wrangling.

1.  Use `lubridate` to create a column called `released` that is a `Date` class. However, to be able to do this, you first need to use `stringr` to search for pattern that matches things like this "(US)\[51\]" in a string like this "September 1, 2006 (US)\[51\]" and removes them. (**Note**: to get full credit, you must create the regular expression).
2.  Use `forcats` to create a factor called `country` (**Note**: you may need to collapse some factor levels).
3.  Transform the `sales` into a unit that is album sales in millions of dollars.
4.  Keep only album sales from the UK, the US or the World.
5.  Auto print your final wrangled tibble data frame.

```{r}
## Step 1: Organizing Data
# Load necessary libraries
library(stringr)
library(lubridate)
library(dplyr)

# Step 2: Define the regular expression in order to detect "(Country) [number]"

pattern <- "\\s*\\([A-Z]{2}\\)\\s*\\[\\d+\\]"

# Step 3: Clean the released column using the regular expression created in step 1

sales <- sales %>%
  mutate(cleaned_released = str_remove(released, pattern))

# Step 4: Convert the cleaned string column into a date class 
sales <- sales %>%
  mutate(released_date = mdy(cleaned_released))  # Use mdy() since the format is "month day, year"

# Step 5: View the Results
print(sales)
```
```{r}
library(forcats)

# Step 6: Create a factor for country
sales <- sales %>%
  mutate(country = factor(country))

#Step 7: Collapse Levels into defined categories
sales <- sales %>%
  mutate(country = fct_collapse(country,
    US = "US",
    UK = "UK",
    World = c("World", "WW"), 
    Other = c("AUS", "JPN", "CAN", "FRA", "FR")))

glimpse(sales)
    
```
```{r}
# Step 8: Transform sales column into a unit that stores album sales in millions of dollars

sales <- sales %>%
  mutate(sales_in_millions = sales / 1e6)  # Divide by a million

filtered_sales <- sales %>%
  filter(country %in% c("US", "UK", "World"))

# Step 9: Auto print as a tibble data frame
print(as_tibble(filtered_sales))
```

## Part 1B

In this section, we will do some more data wrangling followed by summarization using wrangled data from Part 1A.

1.  Keep only album sales from the US.
2.  Create a new column called `years_since_release` corresponding to the number of years since the release of each album from Beyoncé and Taylor Swift. This should be a whole number and you should round down to "14" if you get a non-whole number like "14.12" years. (**Hint**: you may find the `interval()` function from `lubridate` helpful here, but this not the only way to do this.)
3.  Calculate the most recent, oldest, and the median years since albums were released for both Beyoncé and Taylor Swift.

```{r}
# Step 1: Filter from to only keep US Album sales
US_sales <- sales %>%
  filter(country %in% c("US"))

# Check that filtering was successful
print(as_tibble(US_sales))
```
```{r}
# Create a new column called years_since_release

# Step 1: Determine today's date
current_date <- today()

# Step 2: Create a new column for years since release
US_sales <- US_sales %>%
  mutate(years_since_release = floor(as.numeric(interval(released_date, current_date) / years(1))))

# Check updated dataset
glimpse(US_sales)
```
```{r}
# Calculate Summary Statistics
album_stats <- US_sales %>%
  group_by(artist) %>%
  summarize(
    most_recent = max(years_since_release, na.rm = TRUE),
    oldest = min(years_since_release, na.rm = TRUE),
    median_years = median(years_since_release, na.rm = TRUE),
    album_title_most_recent = title[which.max(years_since_release)],  # Title of the most recent album
    album_title_oldest = title[which.min(years_since_release)]  # Title of the oldest album
  )

# Print Results
print(album_stats)
```

## Part 1C

Using the wrangled data from Part 1A:

1.  Calculate the total album sales for each artist and for each `country` (only sales from the UK, US, and World).
    1.  Note: assume that the World sales do not include the UK and US ones.
2.  Using the total album sales, create a [percent stacked barchart](https://r-graph-gallery.com/48-grouped-barplot-with-ggplot2) using `ggplot2` of the percentage of sales of studio albums (in millions) along the y-axis for the two artists along the x-axis colored by the `country`.

```{r}
# Step 1: Calculate the total 

total_sales <- filtered_sales %>%
  group_by(artist, country) %>%
  summarise(total_sales = sum(sales_in_millions))
  
print(total_sales)
```
```{r}
# Creating a percent stacked bar chart in ggplot2
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Step 1: Calculate percentage of sales for each country for each artist
total_sales <- total_sales %>%
  group_by(artist) %>%
  mutate(percentage = total_sales/ sum(total_sales) * 100)

# Step 2: Create a percent stacked bar chart using ggplot2
ggplot(total_sales, aes(x = artist, y = percentage, fill = country)) +
  geom_bar(stat = "identity", position = "fill") + # "fill" for percent stacked bar
  scale_y_continuous(labels = scales::percent_format(scale = 1)) + # Format y-axis as percentages
  labs(
    title = "Percentage of Album Sales by Country",
    x = "Artist",
    y = "Percentage of Total Sales (in millions)",
    fill = "country"
  ) +
  theme_minimal()
```

## Part 1D

Using the wrangled data from Part 1A, use `ggplot2` to create a bar plot for the sales of studio albums (in millions) along the x-axis for each of the album titles along the y-axis.

**Note**:

-   You only need to consider the global World sales (you can ignore US and UK sales for this part). Hint: how would you abbreviate *WorldWide*?
-   The title of the album must be clearly readable along the y-axis.
-   Each bar should be colored by which artist made that album.
-   The bars should be ordered from albums with the most sales (top) to the least sales (bottom) (**Note**: you must use functions from `forcats` for this step).

```{r}
# Step 1: Filter data to only include world sales

world_sales <- filtered_sales %>%
  filter(country=="World") %>%
  group_by(title, artist) %>%
  summarise(total_sales = sum(sales_in_millions, na.rm = TRUE)) %>%
  ungroup()
  
print(world_sales)

# Step 2: Reorder album sales by total sales in descending order
world_sales <- world_sales %>%
  mutate(title = fct_reorder(title, total_sales))

# Step 3: Create the bar plot

ggplot(world_sales, aes(x = total_sales, y = title, fill = artist)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Global Sales of Studio Albums (in Millions)",
    x = "Sales ( in Millions)",
    y = "Album Title",
    fill = "Artist"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10))  # Adjust text size for readability
```

## Part 1E

Using the wrangled data from Part 1A, use `ggplot2` to create a scatter plot of sales of studio albums (in millions) along the y-axis by the released date for each album along the x-axis.

**Note**:

-   The points should be colored by the artist.
-   There should be three scatter plots (one for UK, US and world sales) faceted by rows.

```{r}
# Create Scatter Plot

ggplot(filtered_sales, aes(x = released_date, y = sales_in_millions, color = artist)) +
  geom_point(size = 3) +  # Scatter plot points with size adjustment
  geom_line(size = 1) +   # Add lines connecting the points for each artist
  facet_wrap(~ country, nrow = 3) +  # Facet by Country into three rows
  labs(
    title = "Sales of Studio Albums by Release Date",
    x = "Release Date",
    y = "Sales (in Millions)",
    color = "Artist"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```

# Part 2: Exploring sentiment of lyrics

In Part 2, we will explore the lyrics in the `b_lyrics` and `ts_lyrics` datasets.

## Part 2A

Using `ts_lyrics`, create a new column called `line` with one line containing the character string for each line of Taylor Swift's songs.

-   How many lines in Taylor Swift's lyrics contain the word "hello"? For full credit, show all the rows in `ts_lyrics` that have "hello" in the `line` column and report how many rows there are in total.
-   How many lines in Taylor Swift's lyrics contain the word "goodbye"? For full credit, show all the rows in `ts_lyrics` that have "goodbye" in the `line` column and report how many rows there are in total.

```{r}
# Counting the number of times "hello" appears in Taylor Swift Songs
# Load necessary Libraries

library(dplyr)
library(tidyr)
install.packages("tidytext")
library(tidytext)

# Step 1: Create a new column 'line' for each line of lyrics
ts_lines <- ts_lyrics %>%
  unnest_tokens(line, Lyrics, token = "lines")

# Step 2: Filter for lines containing "hello
hello_lines <- ts_lines %>%
  filter(grepl("hello", line, ignore.case = TRUE)) # Remove case-sensitivity

# Step 3: Display all rows containing hello and count total number of lines with hello
print(hello_lines)  # Display the rows
n_hello_lines <- nrow(hello_lines)  # Count of lines with "hello"
cat("Number of lines containing 'hello':", n_hello_lines, "\n")
```
In total there are 6 lines that contain the word "Hello" in Taylor Swifts Songs

```{r}
# Counting the number of times "goodbye" appears in Taylor Swift Songs
# Filter for lines containing "goodbye"
goodbye_lines <- ts_lines %>%
  filter(grepl("goodbye", line, ignore.case = TRUE))  # Case-insensitive match

# Show all rows containing "goodbye" and count
print(goodbye_lines)  # Display the rows
n_goodbye_lines <- nrow(goodbye_lines)  # Count of lines with "goodbye"
cat("Number of lines containing 'goodbye':", n_goodbye_lines, "\n")
```
In Total there are 12 lines that contain "Goodbye" in Taylor Swift's Songs
## Part 2B

Repeat the same analysis for `b_lyrics` as described in Part 2A.

```{r}
# Counting the number of times "hello" appears in Beyonce Songs
# The Beyonce lyrics file is already separated into lines so there is no need to separate out the lines

# Step 1: Filter for lines containing "hello
hello_lines2 <- b_lyrics %>%
  filter(grepl("hello", line, ignore.case = TRUE)) # Remove case-sensitivity

# Step 2: Display all rows containing hello and count total number of lines with hello
print(hello_lines2)  # Display the rows
n_hello_lines2 <- nrow(hello_lines2)  # Count of lines with "hello"
cat("Number of lines containing 'hello':", n_hello_lines2, "\n")
```
The word "Hello" appears in 91 times in Beyonce songs.

```{r}
# Counting the number of times "Goodbye" appears in Beyonce Songs

# Filter for lines containing "goodbye"
goodbye_lines2 <- b_lyrics%>%
  filter(grepl("goodbye", line, ignore.case = TRUE))  # Case-insensitive match

# Show all rows containing "goodbye" and count
print(goodbye_lines2)  # Display the rows
num_goodbye_lines2 <- nrow(goodbye_lines2)  # Count of lines with "goodbye"
cat("Number of lines containing 'goodbye':", num_goodbye_lines2, "\n")
```
There are 12 lines in Beyonce songs that contain the word "Goodbye".
## Part 2C

Using the `b_lyrics` dataset,

1.  Tokenize each lyrical line by words.
2.  Remove the "stopwords".
3.  Calculate the total number for each word in the lyrics.
4.  Using the "bing" sentiment lexicon, add a column to the summarized data frame adding the "bing" sentiment lexicon.
5.  Sort the rows from most frequent to least frequent words.
6.  Only keep the top 25 most frequent words.
7.  Auto print the wrangled tibble data frame.
8.  Use `ggplot2` to create a bar plot with the top words on the y-axis and the frequency of each word on the x-axis. Color each bar by the sentiment of each word from the "bing" sentiment lexicon. Bars should be ordered from most frequent on the top to least frequent on the bottom of the plot.
9.  Create a word cloud of the top 25 most frequent words.

```{r}
# Load necessary libraries
library(tidytext)
library(dplyr)
library(tidyr)

# Step 1: Token nice each lyrical line by words
b_words <- b_lyrics %>%
  unnest_tokens(word, line, token = "words")

print(b_words)

```
```{r}
# Step 2: Remove the "stopwords"

# Assess the built-in stopwords in the data set using tidytext
data("stop_words")

# Use the anti-join function to remove stopwords from the tokenized data set
b_words_filtered <- b_words %>%
  anti_join(stop_words, by = "word")

#View the tokenized data set without the stopwords
print(b_words_filtered)

# Removing the stopwords reduced the data set by 109,269 lines
```
```{r}
# Step 3: Calculate the total number for each word in the lyrics

b_words_filtered <- b_words_filtered %>%
  count(word, sort = TRUE) # Count the occurrences of each word and sort them in descending order

```
```{r}
# Step 4: Using the "bing" sentiment lexicon add a column to the summarized dataframe 

# Load the "bing" sentiment lexicon from tidytext
bing_sentiments <- get_sentiments("bing")

# Join the bing sentiment lexicon to the tokenized dataset
b_sentiment <- b_words_filtered %>%
  inner_join(bing_sentiments, by = "word")

# View the data frame to check that sentiment annotation is functioning correctly
print(b_sentiment)

```
```{r}
# Step 5: Sort the rows from most frequent to least frequent words
# words are already sorted in descending order after step 3

# Step 6: Only keep the top 25 most frequently used words
# Step 6: Only keep the top 25 most frequently used words
b_sentiment <- b_sentiment %>%
  slice_head(n=25)

# Step 7: Autoprint the truncated b_sentiment dataframe 
b_sentiment
```
```{r}
# Step 8: Create a bar plot of the top words color coded by sentiment
ggplot(b_sentiment, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip coordinates for better readability
  labs(x = "Frequency", y = "Top Words", title = "Most Used words in Beyonce's Songs Colored by Sentiment") +
  theme_minimal() +
  scale_fill_manual(values = c("positive" = "lightblue", "negative" = "lightcoral")) +  # Customize colors for sentiments
  theme(legend.title = element_blank())  # Remove legend title
```
```{r}
# Step 9: Create a Word Cloud of the most frequently used words in Beyonce lyrics

# Install & load necessary package
install.packages("wordcloud2")

library(wordcloud2)

wordcloud2(b_sentiment, 
        size = 1,          # Size of the words in the cloud
        minSize = 0.5,    # Minimum size of the smallest words
        color = 'random-light',  # Color scheme for the words
        backgroundColor = 'white',  # Background color of the word cloud
        shape = 'circle')  # Shape of the word cloud )

```

## Part 2D

Repeat the same analysis as above in Part 2C, but for `ts_lyrics`.

```{r}
# Load necessary libraries
library(tidytext)
library(dplyr)
library(tidyr)

# Step 1: Token nice each lyrical line by words
ts_words <- ts_lines %>%
  unnest_tokens(word, line, token = "words")

print(ts_words)
```
```{r}
# Step 2: Remove the "stopwords"

# Assess the built-in stopwords in the data set using tidytext
data("stop_words")

# Use the anti-join function to remove stopwords from the tokenized data set
ts_words_filtered <- ts_words %>%
  anti_join(stop_words, by = "word")

#View the tokenized data set without the stopwords
print(ts_words_filtered)

# Removing the stopwords reduced the data set by 35,458 lines
```
```{r}
# Step 3: Calculate the total number for each word in the lyrics

ts_words_filtered <- ts_words_filtered %>%
  count(word, sort = TRUE) # Count the occurrences of each word and sort them in descending order

# Step 4: Using the "bing" sentiment lexicon add a column to the summarized dataframe 

# Load the "bing" sentiment lexicon from tidytext
bing_sentiments <- get_sentiments("bing")

# Join the bing sentiment lexicon to the tokenized dataset
ts_sentiment <- ts_words_filtered %>%
  inner_join(bing_sentiments, by = "word")

# View the data frame to check that sentiment annotation is functioning correctly
print(ts_sentiment)
```
```{r}
# Step 5: Sort the rows from most frequent to least frequent words
# words are already sorted in descending order after step 3

# Step 6: Only keep the top 25 most frequently used words
# Step 6: Only keep the top 25 most frequently used words
ts_sentiment <- ts_sentiment %>%
  slice_head(n=25)

# Step 7: Autoprint the truncated b_sentiment dataframe 
ts_sentiment
```
```{r}
# Step 8: Create a bar plot of the top words color coded by sentiment
ggplot(ts_sentiment, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip coordinates for better readability
  labs(x = "Frequency", y = "Top Words", title = "Most Used words in Taylor Swift's Songs Colored by Sentiment") +
  theme_minimal() +
  scale_fill_manual(values = c("positive" = "lightblue", "negative" = "lightcoral")) +  # Customize colors for sentiments
  theme(legend.title = element_blank())  # Remove legend title
```
```{r}
# Step 9: Create a Word Cloud of the most frequently used words in Beyonce lyrics
library(wordcloud2)

wordcloud2(ts_sentiment, 
        size = 1,          # Size of the words in the cloud
        minSize = 0.5,    # Minimum size of the smallest words
        color = 'random-light',  # Color scheme for the words
        backgroundColor = 'white',  # Background color of the word cloud
        shape = 'circle')  # Shape of the word cloud )
```


## Part 2E

Using the `ts_lyrics` dataset,

1.  Tokenize each lyrical line by words.
2.  Remove the "stopwords".
3.  Calculate the total number for each word in the lyrics **for each Album**.
4.  Using the "afinn" sentiment lexicon, add a column to the summarized data frame adding the "afinn" sentiment lexicon.
5.  Calculate the average sentiment score **for each Album**.
6.  Auto print the wrangled tibble data frame.
7.  Join the wrangled data frame from Part 1A (album sales in millions) filtered down to US sales with the wrangled data frame from #6 above (average sentiment score for each album).
8.  Using `ggplot2`, create a scatter plot of the average sentiment score for each album (y-axis) and the album release data along the x-axis. Make the size of each point the album sales in millions.
9.  Add a horizontal line at y-intercept=0.
10. Write 2-3 sentences interpreting the plot answering the question "How has the sentiment of Taylor Swift's albums have changed over time?". Add a title, subtitle, and useful axis labels.

```{r}
# Load necessary libraries
library(dplyr)
library(tidytext)

# Step 1: Tokenize each lyrical line into words, preserving the 'Album' column
ts_words_by_album <- ts_lyrics %>%
  unnest_tokens(word, Lyrics, token = "words")  # Tokenizing Lyrics by words

# Step 2: Remove stopwords
data("stop_words")
ts_words_by_album <- ts_words_by_album %>%
  anti_join(stop_words, by = "word")  # Remove stopwords

print(ts_words_by_album)
```
```{r}
# Step 3: Calculate the total number of occurrences for each word in each Album

word_counts_by_album <- ts_words_by_album %>%
  count(Album, word, name = "total_occurrences", sort = TRUE)

# Merge the word counts back to the original dataset to retain all information
ts_filtered_dataset <- ts_lyrics %>%
  left_join(word_counts_by_album, by = "Album")

# View datsets to ensure merger went well
print(head(ts_lyrics))         # Original dataset
print(head(word_counts_by_album))  # Word counts by album
print(head(ts_filtered_dataset))      # Merged dataset with word counts
```
```{r}
# Step 4: USe the "afinn" sentiment lexicon, and add a column to the summarized dataframe 

install.packages("textdata")
library(textdata)

# Load the Afinn lexicon
afinn <- get_sentiments("afinn")

# Join the Afinn lexicon with the ts_filtered_dataset
ts_filtered_with_sentiment <- ts_filtered_dataset %>%
  left_join(afinn, by = "word") #Join by word to add sentiment scores

# View just the first few rows to ensure join was successful
print(head(ts_filtered_with_sentiment))
```
```{r}
# Step 5: Calculate the average sentiment score for each album

ts_average_sentiment_by_album <- ts_filtered_with_sentiment %>%
  group_by(Album) %>%
  summarize(average_Sentiment = mean(value, na.rm = TRUE)) # Calculate the mean, while ignoring NA values

# Look at the sentiment scores
print(ts_average_sentiment_by_album)

```
```{r}
# Step 7: Join the US sales data set with the average sentiment data set
combined_sales_sentiment <- US_sales %>%
  left_join(ts_average_sentiment_by_album, by = c("title" = "Album")) # Join by Album, the c= specifies by which column since the columns are named differently in each column.

# Check join on combined data set
print(combined_sales_sentiment)

# Filter out data to only keep the taylor swift data
taylor_only_combined <- combined_sales_sentiment %>%
  filter(artist == "Taylor Swift") 

# Check if filtering was successful
print(taylor_only_combined)


```

```{r}
# Step 8: Create a scatter plot of the average sentiment score for each album

ggplot(taylor_only_combined, aes(x = released_date, y = average_Sentiment, size = sales_in_millions, label = title)) +
  geom_point(alpha = 0.7, color = "blue") +  # Set point transparency and color
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Add horizontal line at y=0
  geom_text(vjust = -0.5, size = 3, check_overlap = TRUE) +  # Add album names above points
  scale_size_continuous(name = "Album Sales (in millions)", range = c(3, 10)) +  # Customize size legend
  scale_y_continuous(limits = c(-1, 1)) +  # Adjust y-axis limits to prevent cutoff
  scale_x_date(expand = expansion(mult = c(0.1, 0.1))) +  # Add padding to x-axis
  labs(
    title = "Unpacking Sentiment in Taylor Swift's Discography",
    subtitle = "Relationship Between Album Release Dates and Sentiment Scores",
    x = "Release Date",
    y = "Average Sentiment Score"
  ) +
  theme_minimal() +  # Use a minimal theme for a clean look
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
    
  )

```

The AFINN sentiment lexicon is a list of English words that are rated for emotion value from -5 to +5, sentiment scores can help understand the emotional tone and text of data. It needs to be noted that when applying the AFINN sentiment lexicon to Taylor Swift's discography there are many words that were unable to be coded as a result I was unable to calculate an average sentiment score for reputation. This graph makes it appear that overall the sentiments in Taylor Swifts albums over time began as being neutral in tone. Her first album "Taylor Swift" has the highest average AFINN sentiment score out of all of the albums, however, this score is still only 0.13, while 1989 has the lowest AFINN score with a score of -0.82 the range between those two album scores is fairly right. The scatter plot makes it appear as though over time Taylor Swift's album's have begun to include more negatively toned words, which may be true. However, when you look at the range of scores it's fairly narrow. There doesn't really seem to be an association between sentiment and album sales in the U.S. context. In terms of total sales, maybe because those albums have been around longer the older albums seem to have sold more.But when you factor in that Taylor Swift and Fearless have been re-relased in this dataset that might be a contributing factor. 
# R session information

```{r}
options(width = 120)
sessioninfo::session_info()
```
